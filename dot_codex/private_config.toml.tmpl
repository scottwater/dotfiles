model = "gpt-5.2-codex"
model_reasoning_effort = "high"
tool_output_token_limit = 25000
# Leave room for native compaction near the 272–273k context window.
# Formula: 273000 - (tool_output_token_limit + 15000)
# With tool_output_token_limit=25000 ⇒ 273000 - (25000 + 15000) = 233000
model_auto_compact_token_limit = 233000
web_search = "live"
suppress_unstable_features_warning = true
model_personality = "pragmatic"
[features]
ghost_commit = false
unified_exec = true
apply_patch_freeform = true
skills = true
shell_snapshot = true

[mcp_servers.kapture]
command = "npx"
args = ["-y", "kapture-mcp@latest", "bridge"]


{{- if (get . "kode") }}
[mcp_servers.linear]
url = "https://mcp.linear.app/mcp"
{{- end }}

[projects."/Users/scott/projects"]
trust_level = "trusted"

[projects."/Users/Scott.Watermasysk/projects"]
trust_level = "trusted"

[projects."/Users/scott/work"]
trust_level = "trusted"

[projects."/Users/Scott.Watermasysk/work"]
trust_level = "trusted"

[projects."/Users/scott/src"]
trust_level = "trusted"

[projects."/Users/Scott.Watermasysk/src"]
trust_level = "trusted"
